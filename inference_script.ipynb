{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "from src.transformer_bnftomel import Transformer\n",
    "from utils.f0_utils import get_cont_lf0\n",
    "from utils.load_yaml import HpsYaml\n",
    "\n",
    "from vocoders.hifigan_model import load_hifigan_generator\n",
    "\n",
    "from speaker_encoder import inference as encoder\n",
    "from speaker_encoder.audio import preprocess_wav\n",
    "from data_objects.kaldi_interface import KaldiInterface\n",
    "\n",
    "from src import build_model\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded encoder \"pretrained.pt\" trained to step 1564501\n"
     ]
    }
   ],
   "source": [
    "encoder_speaker_weights = Path(\"/home/grads/q/quamer.waris/projects/Accentron/pretrained_model/pretrained/encoder/saved_models/pretrained.pt\")\n",
    "encoder.load_model(encoder_speaker_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spk_dvec(wav_path):\n",
    "    fpath = Path(wav_path)\n",
    "    wav = preprocess_wav(fpath)\n",
    "    # print('wac-shape',wav.shape)\n",
    "    spk_dvec = encoder.embed_utterance(wav)\n",
    "    #print(spk_dvec)\n",
    "    return spk_dvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transf_model(model_config, model_file, device):\n",
    "    model_class = build_model(model_config[\"model_name\"])\n",
    "    ppg2mel_model = model_class(\n",
    "        model_config[\"model\"]\n",
    "    ).to(device)\n",
    "    ckpt = torch.load(model_file, map_location=device)\n",
    "    ppg2mel_model.load_state_dict(ckpt[\"model\"])\n",
    "    ppg2mel_model.eval()\n",
    "    return ppg2mel_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bnfs(spk_id, utterance_id, kaldi_dir):\n",
    "    ki = KaldiInterface(wav_scp=str(os.path.join(kaldi_dir, 'wav.scp')),\n",
    "                        bnf_scp=str(os.path.join(kaldi_dir, 'bnf/feats.scp')))\n",
    "    bnf = ki.get_feature('_'.join([spk_id, utterance_id]), 'bnf')\n",
    "    return bnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppg2mel_model_train_config = Path('/mnt/data1/waris/projects/dummy/config/transformer_vc_ppg2mel_outspkdloss_inp_conct.yaml')\n",
    "ppg2mel_model_train_config = Path('/mnt/data1/waris/projects/dummy/config/transformer_vc_ppg2mel.yaml')\n",
    "ppg2mel_config = HpsYaml(ppg2mel_model_train_config) \n",
    "#ppg2mel_model_file = Path('/mnt/data1/waris/projects/dummy/ckpt/transformer_baseline/best_loss_step_910000.pth')\n",
    "ppg2mel_model_file = Path('/mnt/data1/waris/projects/dummy/ckpt/transformer_baseline_two/best_loss_step_960000.pth')\n",
    "device = 'cuda'\n",
    "\n",
    "ppg2mel_model = build_transf_model(ppg2mel_config, ppg2mel_model_file, device) \n",
    "hifigan_model = load_hifigan_generator(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def convert(src_speaker_fpath, tgt_speaker_fpath, utterance_id, output_dir):\n",
    "        \n",
    "    # Data related\n",
    "    tgt_speaker = os.path.basename(tgt_speaker_fpath)\n",
    "    tgt_wav_path = f\"{tgt_speaker_fpath}/wav/{utterance_id}.wav\"\n",
    "    tgt_spk_dvec = compute_spk_dvec(tgt_wav_path)\n",
    "    tgt_spk_dvec = torch.from_numpy(tgt_spk_dvec).unsqueeze(0).to(device)\n",
    "\n",
    "    src_speaker = os.path.basename(src_speaker_fpath)\n",
    "    src_speaker_kaldi_dir = os.path.join(src_speaker_fpath, 'kaldi')\n",
    "    ppg = get_bnfs(src_speaker, utterance_id, src_speaker_kaldi_dir)\n",
    "    ppg = torch.from_numpy(ppg).unsqueeze(0).to(device)\n",
    "    \n",
    "    min_len = ppg.shape[1]\n",
    "    ppg = ppg[:, :min_len]\n",
    "\n",
    "    mel_pred, att_ws = ppg2mel_model.inference(torch.squeeze(ppg), torch.squeeze(tgt_spk_dvec))\n",
    "\n",
    "    mel_pred = mel_pred.unsqueeze(0)\n",
    "    \n",
    "    y = hifigan_model(mel_pred.view(1, -1, 80).transpose(1, 2))\n",
    "\n",
    "    step = os.path.basename(ppg2mel_model_file)[:-4].split(\"_\")[-1]\n",
    "    output_dir = os.path.join(output_dir, 'Step_'+step, tgt_speaker)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    wav_fname = f\"{output_dir}/{utterance_id}.wav\"\n",
    "\n",
    "    sf.write(wav_fname, y.squeeze().cpu().numpy(), 24000, \"PCM_16\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthesis for Unseen Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = ['NJS', 'TXHC', 'YKWK', 'ZHAA']\n",
    "#utterance_ids = ['arctic_b0534', 'arctic_b0537', 'arctic_b0538', 'arctic_b0539']\n",
    "utterance_ids = ['arctic_a00'+str(i) for i in range(10, 30)]\n",
    "\n",
    "basepath = '/mnt/data1/waris/datasets/data/arctic_dataset/test_speakers_16k'\n",
    "output_dir = '/mnt/data1/waris/projects/dummy/synthesis_output/baseline_two/'\n",
    "for speaker in speakers:\n",
    "    src_speaker_fpath = os.path.join(basepath, 'BDL')\n",
    "    tgt_speaker_fpath = os.path.join(basepath, speaker)\n",
    "\n",
    "    for utterance_id in utterance_ids:\n",
    "        convert(src_speaker_fpath, tgt_speaker_fpath, utterance_id, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthesis for Seen Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = ['MBMPS', 'BWC', 'HKK', 'SKA']\n",
    "#utterance_ids = ['arctic_b0534', 'arctic_b0537', 'arctic_b0538', 'arctic_b0539']\n",
    "utterance_ids = ['arctic_a00'+str(i) for i in range(10, 30)]\n",
    "\n",
    "basepath = '/mnt/data1/waris/datasets/data/arctic_dataset/all_data_for_ac_vc_train'\n",
    "output_dir = '/mnt/data1/waris/projects/dummy/synthesis_output'\n",
    "for speaker in speakers:\n",
    "    src_speaker_fpath = os.path.join(basepath, 'BDL')\n",
    "    tgt_speaker_fpath = os.path.join(basepath, speaker)\n",
    "\n",
    "    for utterance_id in utterance_ids:\n",
    "        convert(src_speaker_fpath, tgt_speaker_fpath, utterance_id, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSNE Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utterance_ids = ['arctic_b05'+str(i) for i in range(21, 40)]\n",
    "utterance_ids = ['arctic_a00'+str(i) for i in range(10, 50)]\n",
    "speakers = ['NJS', 'TXHC', 'YKWK', 'ZHAA', 'BDL']\n",
    "speakers_fac = ['FAC_NJS', 'FAC_TXHC', 'FAC_YKWK', 'FAC_ZHAA']\n",
    "basepath = '/mnt/data1/waris/datasets/data/arctic_dataset/test_speakers_16k'\n",
    "\n",
    "embed_unseen = []\n",
    "label_unseen = []\n",
    "for speaker in speakers:\n",
    "    for utterance_id in utterance_ids:\n",
    "        tgt_speaker_fpath = f\"{basepath}/{speaker}/wav/{utterance_id}.wav\"\n",
    "        tgt_spk_dvec = compute_spk_dvec(tgt_speaker_fpath)\n",
    "        embed_unseen.append(tgt_spk_dvec)\n",
    "        label_unseen.append(speaker)\n",
    "\n",
    "basepath = '/mnt/data1/waris/projects/dummy/synthesis_output/Step_1380000'\n",
    "#utterance_ids = ['arctic_b0534', 'arctic_b0537', 'arctic_b0538', 'arctic_b0539']\n",
    "utterance_ids = ['arctic_a00'+str(i) for i in range(10, 30)]\n",
    "\n",
    "for speaker in speakers:\n",
    "    if speaker == 'BDL':\n",
    "        continue\n",
    "    for utterance_id in utterance_ids:\n",
    "        tgt_speaker_fpath = f\"{basepath}/{speaker}/{utterance_id}.wav\"\n",
    "        tgt_spk_dvec = compute_spk_dvec(tgt_speaker_fpath)\n",
    "        embed_unseen.append(tgt_spk_dvec)\n",
    "        label_unseen.append(\"FAC_\"+speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing t-SNE embedding - speaker\n"
     ]
    }
   ],
   "source": [
    "from sklearn import (manifold, datasets, decomposition, ensemble, discriminant_analysis, random_projection)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Computing t-SNE\n",
    "print(\"Computing t-SNE embedding - speaker\")\n",
    "tsne_sp = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "speaker_tsne = tsne_sp.fit_transform(embed_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "markers = [\"d\" , \"o\", \"^\", \"P\", \"p\", \"X\", \"*\", \"s\", \"v\"]\n",
    "speakers = speakers + speakers_fac\n",
    "\n",
    "colors =  mpl.cm.get_cmap('tab20')(np.arange(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 2) NJS\n",
      "(40, 2) TXHC\n",
      "(40, 2) YKWK\n",
      "(40, 2) ZHAA\n",
      "(40, 2) BDL\n",
      "(20, 2) FAC_NJS\n",
      "(20, 2) FAC_TXHC\n",
      "(20, 2) FAC_YKWK\n",
      "(20, 2) FAC_ZHAA\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "for speaker, c, m in zip(speakers, colors, markers):\n",
    "    X_speaker_embedding = speaker_tsne[np.where(speaker==np.array(label_unseen))]\n",
    "    print(X_speaker_embedding.shape, speaker)\n",
    "    plt.scatter(X_speaker_embedding[:,0], X_speaker_embedding[:,1], label=speaker, marker=m, color=c)\n",
    "    plt.text(X_speaker_embedding[-1,0], X_speaker_embedding[-1,1], speaker)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"embed_viz/SpeakerEmbeddings_Unseen_960k_baseline_two.png\", format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utterance_ids = ['arctic_b04'+str(i) for i in range(21, 40)]\n",
    "utterance_ids = ['arctic_a00'+str(i) for i in range(10, 50)]\n",
    "speakers = ['MBMPS', 'BWC', 'HKK', 'SKA', \"BDL\"]\n",
    "speakers_fac = ['FAC_MBMPS', 'FAC_BWC', 'FAC_HKK', 'FAC_SKA']\n",
    "basepath = '/mnt/data1/waris/datasets/data/arctic_dataset/all_data_for_ac_vc_train'\n",
    "\n",
    "embed_seen = []\n",
    "label_seen = []\n",
    "for speaker in speakers:\n",
    "    for utterance_id in utterance_ids:\n",
    "        tgt_speaker_fpath = f\"{basepath}/{speaker}/wav/{utterance_id}.wav\"\n",
    "        tgt_spk_dvec = compute_spk_dvec(tgt_speaker_fpath)\n",
    "        embed_seen.append(tgt_spk_dvec)\n",
    "        label_seen.append(speaker)\n",
    "\n",
    "basepath = '/mnt/data1/waris/projects/dummy/synthesis_output/Step_1380000'\n",
    "#utterance_ids = ['arctic_b0534', 'arctic_b0537', 'arctic_b0538', 'arctic_b0539']\n",
    "utterance_ids = ['arctic_a00'+str(i) for i in range(10, 30)]\n",
    "\n",
    "for speaker in speakers:\n",
    "    if speaker == 'BDL':\n",
    "        continue\n",
    "    for utterance_id in utterance_ids:\n",
    "        tgt_speaker_fpath = f\"{basepath}/{speaker}/{utterance_id}.wav\"\n",
    "        tgt_spk_dvec = compute_spk_dvec(tgt_speaker_fpath)\n",
    "        embed_seen.append(tgt_spk_dvec)\n",
    "        label_seen.append(\"FAC_\"+speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing t-SNE embedding - speaker\n"
     ]
    }
   ],
   "source": [
    "from sklearn import (manifold, datasets, decomposition, ensemble, discriminant_analysis, random_projection)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Computing t-SNE\n",
    "print(\"Computing t-SNE embedding - speaker\")\n",
    "tsne_sp = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "speaker_tsne = tsne_sp.fit_transform(embed_seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "markers = [\"d\" , \"o\", \"^\", \"P\", \"p\", \"X\", \"*\", \"s\", \"v\"]\n",
    "speakers = speakers + speakers_fac\n",
    "\n",
    "colors =  mpl.cm.get_cmap('tab20')(np.arange(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 2) MBMPS\n",
      "(40, 2) BWC\n",
      "(40, 2) HKK\n",
      "(40, 2) SKA\n",
      "(40, 2) BDL\n",
      "(20, 2) FAC_MBMPS\n",
      "(20, 2) FAC_BWC\n",
      "(20, 2) FAC_HKK\n",
      "(20, 2) FAC_SKA\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "for speaker, c, m in zip(speakers, colors, markers):\n",
    "    X_speaker_embedding = speaker_tsne[np.where(speaker==np.array(label_seen))]\n",
    "    print(X_speaker_embedding.shape, speaker)\n",
    "    plt.scatter(X_speaker_embedding[:,0], X_speaker_embedding[:,1], label=speaker, marker=m, color=c)\n",
    "    plt.text(X_speaker_embedding[-1,0], X_speaker_embedding[-1,1], speaker)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"embed_viz/SpeakerEmbeddings_Seen_1380000.png\", format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d77718d5a62f7cd3ca9f776720549c002798c1dd777036252f1ac346e8dce97b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('transformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
