data:
  train_fid_list: "/mnt/data2/bhanu/code-bases/ppg-vc/train.txt"
  dev_fid_list: "/mnt/data2/bhanu/code-bases/ppg-vc/dev.txt" 
  eval_fid_list: "/home/shaunxliu/data/vctk/fidlists/eval_fidlist.txt"
  arctic_ppg_dir: "/mnt/data2/bhanu/datasets/bfn_feats"
  arctic_f0_dir: "/mnt/data2/bhanu/datasets/f0"
  arctic_wav_dir: "/mnt/data2/bhanu/datasets/all_data_for_ac_vc"
  arctic_spk_dvec_dir: "/mnt/data2/bhanu/datasets/dvec/GE2E_spkEmbed_step_5805000"
  ppg_file_ext: "ling_feat.npy"
  f0_file_ext: "f0.npy"
  wav_file_ext: "wav"
  min_max_norm_mel: true
  mel_min: -12.0
  mel_max: 2.5
  pretrain_model_file: /mnt/data2/bhanu/code-bases/ppg-vc/ckpt/convpos_spkloss_inputconc/step_479000.pth

# hparas:
#   batch_size: 24
#   valid_step: 1000
#   max_step: 1000000
#   optimizer: 'Adam'
#   lr: 0.001
#   eps: 1.0e-8
#   weight_decay: 1.0e-6
#   lr_scheduler: 'warmup'   # "fixed", "warmup"
hparas:
  batch_size: 8
  valid_step: 1000
  max_step: 1000000
  optimizer: 'Lamb'
  lr: 0.001
  eps: 0
  weight_decay: 0
  lr_scheduler:    # "fixed", "warmup"

model_name: "transf"
model:
  input_size: 146    # 144 ppg_dim and 2 pitch 
  multi_spk: True
  use_spk_dvec: True  # for one_shot VC

  idim: 144
  odim : 80
  whereusespkd: atinput
  eprenet_conv_layers: 0  # one more linear layer w/o non_linear will be added for 0_centor
  eprenet_conv_filts: 0
  eprenet_conv_chans: 0
  dprenet_layers: 2  # one more linear layer w/o non_linear will be added for 0_centor
  dprenet_units: 256
  adim: 384
  aheads: 4
  elayers: 6
  eunits: 1536
  dlayers: 6
  dunits: 1536
  postnet_layers: 5
  postnet_filts: 5
  postnet_chans: 256
  use_masking: True
  bce_pos_weight: 5.0
  use_batch_norm: True
  use_scaled_pos_enc: False
  encoder_normalize_before: True
  decoder_normalize_before: False
  encoder_concat_after: False
  decoder_concat_after: False
  spk_embed_dim: 256
  spk_embed_integration_type : concat
  reduction_factor: 1
  encoder_reduction_factor: 1
  decoder_reduction_factor: 1
  use_scaled_pos_enc: False
  transformer_input_layer: conv2d-scaled-pos-enc
  loss_type : L2

  # minibatch related
  batch_sort_key: input # shuffle or input or output
  batch_bins: 3340800 

  # training related
  transformer_init: pytorch
  transformer_warmup_steps: 4000
  transformer_lr: 0.1
  initial_encoder_alpha: 1.0
  initial_decoder_alpha: 1.0
  eprenet_dropout_rate: 0.0
  dprenet_dropout_rate: 0.5
  postnet_dropout_rate: 0.5
  transformer_enc_dropout_rate: 0.1
  transformer_enc_positional_dropout_rate: 0.1
  transformer_enc_attn_dropout_rate: 0.1
  transformer_dec_dropout_rate: 0.1
  transformer_dec_positional_dropout_rate: 0.1
  transformer_dec_attn_dropout_rate: 0.1
  transformer_enc_dec_attn_dropout_rate: 0.1
  use_guided_attn_loss: true
  num_heads_applied_guided_attn: 2
  num_layers_applied_guided_attn: 2
  modules_applied_guided_attn: ["encoder-decoder"]
  guided_attn_loss_lambda: 10
  enc_init_mods: encoder
  dec_init_mods: decoder,postnet,feat_out,prob_out
  positionwise_layer_type : conv1d
  positionwise_conv_kernel_size : 1
  use_weighted_masking : False
  guided_attn_loss_sigma: 0.4  # sigma of guided attention loss
  guided_attn_loss_lambda: 1.0 # strength of guided attention loss
  # pretrained_model : /mnt/data2/bhanu/code-bases/ppg-vc/ckpt/convpos_5split/step_258000.pth
  
